# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RpA38Yq0_l9RkRzBhtRCrgmK5i5Gu4NB
"""

!pip install gradio==4.19.2 sentence-transformers faiss-cpu PyMuPDF transformers accelerate

# ---------------------------
# IMPORTS
# ---------------------------
import fitz  # PyMuPDF
import faiss
import numpy as np
import gradio as gr
from sentence_transformers import SentenceTransformer
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

# ---------------------------
# LOAD EMBEDDING MODEL
# ---------------------------
print("Loading embedding model...")
embed_model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

# ---------------------------
# LOAD IBM GRANITE 3.3 2B INSTRUCT
# ---------------------------
print("Loading IBM Granite 3.3 2B Instruct LLM...")
model_name = "ibm-granite/granite-3.3-2b-instruct"

tokenizer = AutoTokenizer.from_pretrained(model_name)
llm_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)

generator = pipeline(
    "text-generation",
    model=llm_model,
    tokenizer=tokenizer,
    max_new_tokens=300,
    temperature=0.3
)

# ---------------------------
# GLOBAL VARIABLES
# ---------------------------
documents = []
faiss_index = None
all_chunks = []

# ---------------------------
# PDF PROCESSING
# ---------------------------
def extract_text_from_pdf(pdf_bytes):
    doc = fitz.open(stream=pdf_bytes, filetype="pdf")
    text = ""
    for page in doc:
        text += page.get_text()
    return text

def chunk_text(text, chunk_size=500):
    words = text.split()
    chunks = []
    for i in range(0, len(words), chunk_size):
        chunk = " ".join(words[i:i+chunk_size])
        chunks.append(chunk)
    return chunks

# ---------------------------
# BUILD VECTOR STORE
# ---------------------------
def build_faiss_index(all_chunks):
    embeddings = embed_model.encode(all_chunks)
    index = faiss.IndexFlatL2(embeddings.shape[1])
    index.add(np.array(embeddings).astype("float32"))
    return index

# ---------------------------
# RETRIEVAL
# ---------------------------
def retrieve_relevant_chunks(query, k=3):
    global faiss_index, all_chunks
    query_emb = embed_model.encode([query])
    D, I = faiss_index.search(np.array(query_emb).astype("float32"), k)
    return [all_chunks[i] for i in I[0]]

# ---------------------------
# LLM ANSWER GENERATION
# ---------------------------
def generate_answer(query, retrieved_chunks):
    context = "\n".join(retrieved_chunks)
    prompt = (
        f"You are an academic assistant. Use ONLY the context to answer.\n\n"
        f"Context:\n{context}\n\n"
        f"Question: {query}\n"
        f"Answer:"
    )

    output = generator(prompt)[0]["generated_text"]
    return output[len(prompt):].strip()

# ---------------------------
# MAIN PIPELINE
# ---------------------------
def process_pdfs(files):
    """
    This function processes the uploaded PDFs,
    extracts text, chunks them, and builds FAISS.
    """
    global documents, faiss_index, all_chunks

    documents = []
    all_chunks = []

    for f in files:
        pdf_bytes = f.read()
        text = extract_text_from_pdf(pdf_bytes)
        chunks = chunk_text(text)
        all_chunks.extend(chunks)

    faiss_index = build_faiss_index(all_chunks)

    return f"âœ… PDFs processed successfully. Total chunks: {len(all_chunks)}"


def answer_question(query):
    if not all_chunks:
        return "âš  Please upload PDFs first."

    retrieved = retrieve_relevant_chunks(query)
    answer = generate_answer(query, retrieved)
    return answer

# ---------------------------
# GRADIO INTERFACE
# ---------------------------
with gr.Blocks() as app:
    gr.Markdown("<h2>ðŸ“˜ StudyMate â€” AI Academic Assistant (Gradio Version)</h2>")

    with gr.Tab("Upload PDFs"):
        upload = gr.File(label="Upload PDF Files", file_count="multiple", file_types=["pdf"])
        process_btn = gr.Button("Process PDFs")
        output1 = gr.Textbox(label="Status")
        process_btn.click(process_pdfs, inputs=upload, outputs=output1)

    with gr.Tab("Ask Questions"):
        question = gr.Textbox(label="Enter your question")
        ask_btn = gr.Button("Get Answer")
        answer_box = gr.Textbox(label="Answer", lines=10)
        ask_btn.click(answer_question, inputs=question, outputs=answer_box)

app.launch()